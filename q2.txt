
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("DataCleaning").getOrCreate()

# Load dataset (replace 'path_to_file' with the actual path)
df = spark.read.csv("Sales Data.csv", header=True, inferSchema=True)

# Display the loaded DataFrame
df.show()
# Verify column names and data types
df.printSchema()

# Count null values in each column
from pyspark.sql.functions import col, isnan, when, count

df.select(
    [count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns]).show()

# Decide whether to fill or drop rows with missing values
# For numerical columns, replace nulls with mean values
from pyspark.sql.functions import mean

numerical_columns = ['Sales', 'Quantity Ordered']
for col_name in numerical_columns:
    mean_value = df.select(mean(col_name)).collect()[0][0]
    df = df.fillna({col_name: mean_value})

# Drop rows with null values in critical columns
df = df.dropna()
df = df.dropDuplicates()
# Cast numerical columns to appropriate types
df = df.withColumn("Sales", col("Sales").cast("float"))
df = df.withColumn("Quantity Ordered", col("Quantity Ordered").cast("integer"))
df = df.withColumn("Price Each", col("Price Each").cast("float"))

# Verify schema
df.printSchema()
# Remove rows with negative values in specific columns
columns_to_check = ['Sales', 'Price Each', 'Quantity Ordered']
for col_name in columns_to_check:
    df = df.filter(col(col_name) >= 0)

# Calculate total sales per product
df.groupBy("Product").sum("Sales").withColumnRenamed("sum(Sales)", "Total Sales").show()
